{"generator": "bdist_wheel (0.24.0)", "summary": "Chinese Words Segementation Utilities", "metadata_version": "2.0", "name": "jieba", "version": "0.39", "extensions": {"python.details": {"project_urls": {"Home": "https://github.com/fxsjy/jieba"}, "contacts": [{"email": "ccnusjy@gmail.com", "name": "Sun, Junyi", "role": "author"}], "document_names": {"description": "DESCRIPTION.rst"}}}, "license": "MIT", "keywords": ["NLP", "tokenizing", "Chinese", "word", "segementation"], "classifiers": ["Intended Audience :: Developers", "License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Natural Language :: Chinese (Simplified)", "Natural Language :: Chinese (Traditional)", "Programming Language :: Python", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.6", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.2", "Programming Language :: Python :: 3.3", "Programming Language :: Python :: 3.4", "Topic :: Text Processing", "Topic :: Text Processing :: Indexing", "Topic :: Text Processing :: Linguistic"]}